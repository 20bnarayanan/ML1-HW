---
title: "401-hw3"
output: html_document
date: "2024-10-14"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r p1q1}
library(readr)
library(car)
library(carData)
```

## Problem 2

a) Here, \( y \) depends on both \( x \) and \( w \), and \( x \) depends on \( w \). This configuration represents a **fork** because \( w \) affects both \( x \) and \( y \).

```{r p1q2}
# Set seed for reproducibility
set.seed(123)

# Generate the data
n <- 500
w <- runif(n, min = 0, max = 5)
delta <- rnorm(n, mean = 0, sd = 1)
x <- w + delta
epsilon <- rnorm(n, mean = 0, sd = 1)
y <- 4 + 2 * x - 3 * w + epsilon

# Correlation matrix
data <- data.frame(x = x, w = w, y = y)
cor_matrix <- cor(data)

# Basic descriptive statistics
summary_stats <- summary(data)

list(correlation_matrix = cor_matrix, summary_statistics = summary_stats)

```


```{r p1q3}
# Linear regression of y on x
model1 <- lm(y ~ x, data = data)

# Summary of the model to check coefficients
summary(model1)

# Extracting 95% confidence interval for the coefficient of x
confint(model1, "x", level = 0.95)

```

c) The large p-value of 0.988 means we fail to reject the null hypothesis that the coefficient of x is 0, which suggests that the coefficient of x is not statistically significant at the 0.05 significance level. The 95% CI (-0.1339717, 0.1320145) does not cover the true slope of 2 for x.


```{r p1q4}

# Linear regression of y on x and w
model2 <- lm(y ~ x + w, data = data)

# Summary of the model to check coefficients
summary(model2)

# 95% confidence interval for the coefficient of x in the second model
confint(model2, "x", level = 0.95)

```

d) The small p-value that is < 2.2e-16 means we can reject the null hypothesis that the coefficient of x is 0, which suggests that the coefficient of x is significant at the 0.05 significance level. The 95% CI (1.900471, 2.078544) cover the true slope of 2 for x.


```{r p1q5}
# Calculate VIF
vif_values <- vif(model2)
vif_values

```

e) The VIF for x is 3.036348 and the VIF for w is also 3.036348.


## Problem 3

a) In this case, \( w \) depends on \( y \), which depends on \( x \). This is a **collider** structure since \( y \) is an effect of both \( x \) and \( w \).

```{r p2q1}

# Set seed for reproducibility
set.seed(123)

# Generate the data
n <- 500
x <- runif(n, min = 0, max = 5)
delta <- rnorm(n, mean = 0, sd = 1)
y <- x + delta
epsilon <- rnorm(n, mean = 0, sd = 1)
w <- 4 + 2 * x + 3 * y + epsilon

# Correlation matrix
data <- data.frame(x = x, y = y, w = w)
cor_matrix <- cor(data)

# Basic descriptive statistics
summary_stats <- summary(data)

list(correlation_matrix = cor_matrix, summary_statistics = summary_stats)


```


```{r p2q2}

# Linear regression of y on x
model1 <- lm(y ~ x, data = data)

# Summary of the model to check coefficients
summary(model1)

# Extracting 95% confidence interval for the coefficient of x
confint(model1, "x", level = 0.95)

```

c) The small p-value that is < 2.2e-16 means we can reject the null hypothesis that the coefficient of x is 0, which suggests that the coefficient of x is significant at the 0.05 significance level. The 95% CI (0.9465387, 1.071016) cover the true slope of 1 for x.


```{r p2q3}

# Linear regression of y on x and w
model2 <- lm(y ~ x + w, data = data)

# Summary of the model to check coefficients
summary(model2)

# 95% confidence interval for the coefficient of x in the second model
confint(model2, "x", level = 0.95)

```


d) The small p-value that is < 2.2e-16 means we can reject the null hypothesis that the coefficient of x is 0, which suggests that the coefficient of x is significant at the 0.05 significance level. The 95% CI (-0.547967, -0.4497033) does not cover the true slope of 1 for x.


```{r p2q4}

# Calculate VIF
vif_values <- vif(model2)
vif_values

```

e) The VIF for x is 6.067631 and the VIF for w is also 6.067631.


```{r p2q5}

# R-squared and residual standard error for both models
r_squared_model1 <- summary(model1)$r.squared
se_model1 <- summary(model1)$sigma

r_squared_model2 <- summary(model2)$r.squared
se_model2 <- summary(model2)$sigma

list(
  model1 = list(R_squared = r_squared_model1, Residual_SE = se_model1),
  model2 = list(R_squared = r_squared_model2, Residual_SE = se_model2)
)

```
f) Model 2 is the better model as it explains more of the variability in y and has lower prediction error on average. Since the VIF values from Model 2 are around 3.04 for both predictors, multicollinearity is present but moderate. This suggests some dependence between predictors, which could affect coefficient stability and interpretability, although it isnâ€™t excessively high.


## Problem 4

a) In this case, \( w \) is influenced by \( x \), and \( y \) depends on \( w \). This configuration resembles a **pipe**, where the relationship flows from \( x \) to \( w \) and then to \( y \).


```{r p3q2}
# Set seed for reproducibility
set.seed(123)

# Generate the data
n <- 500
x <- runif(n, min = 0, max = 5)
delta <- rnorm(n, mean = 0, sd = 1)
w <- x + delta
epsilon <- rnorm(n, mean = 0, sd = 1)
y <- 2 * w + epsilon

# Correlation matrix
data <- data.frame(x = x, w = w, y = y)
cor_matrix <- cor(data)

# Basic descriptive statistics
summary_stats <- summary(data)

list(correlation_matrix = cor_matrix, summary_statistics = summary_stats)


```


```{r p3q3}

# Linear regression of y on x
model1 <- lm(y ~ x, data = data)

# Summary of the model to check coefficients
summary(model1)

# Extracting 95% confidence interval for the coefficient of x
confint(model1, "x", level = 0.95)

```

c) The small p-value that is < 2.2e-16 means we can reject the null hypothesis that the coefficient of x is 0, which suggests that the coefficient of x is significant at the 0.05 significance level.


```{r p3q4}

# Linear regression of y on x and w
model2 <- lm(y ~ x + w, data = data)

# Summary of the model to check coefficients
summary(model2)

```

d) The large p-value of 0.915 means we cannot reject the null hypothesis that the coefficient of x is 0, which suggests that the coefficient of x is not significant at the 0.05 significance level.

e) The R squared of the first model is 0.6196, and the R squared of the second model is 0.922. Therefore, Model 2 is the better model as it explains more of the variability in y on average.

